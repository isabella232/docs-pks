---
title: Tanzu Kubernetes Grid Integrated Edition Architecture
owner: TKGI
---

This topic describes how <%= vars.product_full %> manages the deployment of Kubernetes clusters.

##<a id="overview"></a><%= vars.product_short %> Overview

An <%= vars.product_short %> environment consists of a <%= vars.k8s_runtime_abbr %> Control Plane 
and one or more workload clusters.  

<%= vars.product_short %> administrators use the <%= vars.k8s_runtime_abbr %> Control Plane to 
deploy and manage Kubernetes clusters. The workload clusters run the apps pushed by developers.  

The following illustrates the interaction between <%= vars.product_short %> components:  
<br>
<%= image_tag('images/components.png') %>
<%# Image source: https://docs.google.com/drawings/d/1TZkaTSCiddEE7mZtOTjTg6jBuDAy0D3CI9JY56HBIAY/edit %>

Administrators access the <%= vars.k8s_runtime_abbr %> Control Plane 
through the <%= vars.k8s_runtime_abbr %> Command Line Interface (<%= vars.k8s_runtime_abbr %> CLI) installed on their local workstations.  

Within the <%= vars.k8s_runtime_abbr %> Control Plane the <%= vars.control_plane %> and <%= vars.k8s_runtime_abbr %> Broker use BOSH to execute the requested cluster management functions. 
For information about the <%= vars.k8s_runtime_abbr %> Control Plane, see [<%= vars.k8s_runtime_abbr %> Control Plane Overview](#control-plane) below. 
For instructions on installing the <%= vars.k8s_runtime_abbr %> CLI, see [Installing the <%= vars.k8s_runtime_abbr %> CLI](installing-cli.html).  

Kubernetes deploys and manages workloads on Kubernetes clusters.
Administrators use the  Kubernetes CLI, `kubectl`, to direct Kubernetes 
from their local workstations. 
For information about `kubectl`, see [Overview of kubectl]
(https://kubernetes.io/docs/reference/kubectl/overview/) in the Kubernetes documentation.  

##<a id="cluster-management"></a><a id="control-plane"></a><%= vars.k8s_runtime_abbr %> Control Plane Overview

The <%= vars.k8s_runtime_abbr %> Control Plane manages the lifecycle of Kubernetes clusters deployed 
using <%= vars.product_short %>. 

The control plane provides the following via the <%= vars.control_plane %>:

* View cluster plans
* Create clusters
* View information about clusters
* Obtain credentials to deploy workloads to clusters
* Scale clusters
* Delete clusters
* Create and manage network profiles for VMware NSX-T

In addition, the <%= vars.k8s_runtime_abbr %> Control Plane can upgrade all existing clusters using the **Upgrade all clusters** BOSH errand.
For more information, see [Upgrade Kubernetes Clusters](upgrade.html#upgrade-instances) in _Upgrading <%= vars.product_short %> (Flannel Networking)_.

<br>
<%= vars.k8s_runtime_abbr %> Control Plane is hosted on a pair of VMs:

* The [<%= vars.control_plane %> VM](#tkgi-api-vm) hosts cluster management services.
* The [<%= vars.control_plane_db %> VM](#pks-db-vm) stores cluster management data. 

###<a id="tkgi-api-vm"></a><%= vars.control_plane %> VM

The <%= vars.control_plane %> VM hosts the following services:

* User Account and Authentication (UAA)  
* <%= vars.control_plane %>  
* <%= vars.k8s_runtime_abbr %> Broker
* Billing and Telemetry 

The following sections describe UAA, <%= vars.control_plane %>, and <%= vars.k8s_runtime_abbr %> Broker services,
the primary services hosted on the <%= vars.control_plane %> VM.

####<a id="uaa"></a>UAA

When a user logs in to or logs out of the <%= vars.control_plane %> through the <%= vars.k8s_runtime_abbr %> CLI, the <%= vars.k8s_runtime_abbr %> CLI communicates with UAA to authenticate them.
The <%= vars.control_plane %> permits only authenticated users to manage Kubernetes clusters.
For more information about authenticating, see [<%= vars.control_plane %> Authentication](api-auth.html).

UAA must be configured with the appropriate users and user permissions.
For more information, see [Managing <%= vars.product_short %> Users with UAA](manage-users.html).

####<a id="tkgi-api"></a><%= vars.control_plane %>

Through the <%= vars.k8s_runtime_abbr %> CLI, users instruct the <%= vars.control_plane %> service to deploy, scale up, and delete Kubernetes clusters as well as show cluster details and plans.
The <%= vars.control_plane %> can also write Kubernetes cluster credentials to a local kubeconfig file, which enables users to connect to a cluster through `kubectl`.

On AWS, GCP, and vSphere without NSX-T deployments the <%= vars.k8s_runtime_abbr %> CLI communicates with the 
<%= vars.control_plane %> within the control plane via the <%= vars.control_plane %> Load Balancer. 
On vSphere with NSX-T deployments the <%= vars.control_plane %> host is accessible via a DNAT rule. 
For information about enabling the <%= vars.control_plane %> on vSphere with NSX-T, see the 
[Share the <%= vars.control_plane %> Endpoint](installing-nsx-t.html#retrieve-endpoint) section in 
_Installing <%= vars.product_short %> on vSphere with NSX-T Integration_.

The <%= vars.control_plane %> sends all cluster management requests, except read-only requests, to the <%= vars.k8s_runtime_abbr %> Broker.

####<a id="tkgi-broker"></a><%= vars.k8s_runtime_abbr %> Broker

When the <%= vars.control_plane %> receives a request to modify a Kubernetes cluster, it instructs the <%= vars.k8s_runtime_abbr %> Broker to make the requested change.

The <%= vars.k8s_runtime_abbr %> Broker consists of an [On-Demand Service Broker](https://docs.pivotal.io/svc-sdk/odb/index.html) and a Service Adapter. The <%= vars.k8s_runtime_abbr %> Broker generates a BOSH manifest and instructs the BOSH Director to deploy or delete the Kubernetes cluster.

For <%= vars.product_short %> deployments on vSphere with NSX-T, there is an additional component, the <%= vars.product_short %> NSX-T Proxy Broker.
The <%= vars.control_plane %> communicates with the <%= vars.k8s_runtime_abbr %> NSX-T Proxy Broker, which in turn communicates with the NSX Manager to provision the Node Networking resources.
The <%= vars.k8s_runtime_abbr %> NSX-T Proxy Broker then forwards the request to the On-Demand Service Broker to deploy the cluster.

###<a id="pks-db-vm"></a><%= vars.control_plane_db %> VM

The <%= vars.control_plane_db %> VM hosts MySQL, proxy, and other data-related services. 
These data-related functions persist <%= vars.k8s_runtime_abbr %> Control Plane data for the the following services:

* <%= vars.control_plane %>
* UAA
* Billing
* Telemetry

##<a id="azs"></a>Availability Zones

<%= vars.product_short %> uses Availability Zones (AZs) to provide high availability for Kubernetes cluster workers.

When an operator creates Plans for developers, they assign AZs to the Plans. 
Assigning multiple AZs to a Plan allows developers to provide high-availability for their worker clusters. 
When a cluster has more than one node, 
Ops Manager balances those nodes across the Availability Zones assigned to the cluster. 

Public-cloud IaaSes such as AWS and Azure provide AZs as part of their service.
In vSphere with NSX-T, you define and create AZs using vCenter clusters and resource pools.
See [Step 4: Create Availability Zones](./vsphere-nsxt-om-config.html#create-az) in _Configuring BOSH Director with NSX-T for Tanzu Kubernetes Grid Integrated Edition_ for how to create AZs in NSX-T.

For instructions on selecting AZs for your <%= vars.product_short %> Plans, 
see [Plans](installing-nsx-t.html#plans) 
in _Installing Tanzu Kubernetes Grid Integrated Edition on vSphere with NSX-T_.  

For instructions on selecting the AZ for the <%= vars.product_short %> control plane, 
see [Assign AZs and Networks](installing-nsx-t.html#azs-networks) 
in _Installing Tanzu Kubernetes Grid Integrated Edition on vSphere with NSX-T_.  


##<a id='windows-ha'></a>Windows Worker-Based Kubernetes Cluster High Availability

Windows worker-based cluster Linux nodes can be configured in either standard or high availability modes.

* In standard mode, a single Master/etcd node and a single Linux worker manage a cluster's Windows Kubernetes VMs.  
* In high availability mode, 
multiple Master/etcd and Linux worker nodes manage a cluster's Windows Kubernetes VMs.  

The following illustrates the interaction between the 
<%= vars.product_short %> Management Plane and Windows worker-based Kubernetes clusters:  
<br>
<%= image_tag('images/overview-windows-linux-workers.png') %>
<%# Image source: https://docs.google.com/drawings/d/14toxchQDwvugX1PNnw9wQTzYBe3HmmeqcNWyswiEkPg/edit %>
<br>
To configure <%= vars.product_short %> Windows worker-based clusters for high availability, set these fields in the **Plan** pane as described in [Plans](windows-workers.html#plans) in _Configuring Windows Worker-Based Kubernetes Clusters_:

* **Enable HA Linux workers**
* **Master/ETCD Node Instances**
* **Worker Node Instances**
